{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\Blue Light Blue Color Blocks Flight Attendant CV.pdf', 'page': 0, 'page_label': '1'}, page_content='AMINA IQBAL \\nML ENGINEER \\n+923232983818 amminaiqball@gmail.com\\nPORTFOLIO\\nTHANKS A LOT\\nPROFILE\\nA passionate Machine Learning intern with hands-on experience in\\nbuilding ML models and developing interactive applications using\\nStreamlit and LangChain. Successfully created chatbots and integrated\\nML models into Streamlit applications. Currently working on a Retrieval-\\nAugmented Generation (RAG) system, leveraging advanced AI\\ntechniques for efficient data retrieval and generation. Eager to apply and\\nexpand my knowledge in AI and ML through innovative projects.\\nMSc in Math & Physics\\nUniversity of Punjab\\n2019 - 2023\\nGovt. Degree College for Women\\nIntermidiate in Engeenring\\n2016 - 2018\\nEDUCATION\\n SKILLS\\nEnglish\\nUrdu\\nLANGUAGE\\nMachine Learning\\nRetrieval Augmented\\nGeneration\\nDeep Learning (Basic)\\nhttps://www.linkedin.com/in/amminaiqball/\\nArtificial Intelligence,\\nPython\\n Libraries, pandas,\\nnumpy, Seaborn,\\nMatplotlib, Scikit Learn, \\nData Analysis,\\nData Visualization,\\nProject Management,\\nCommunication, \\nCanva\\n Thank you for your time and\\nconsideration. \\nDocu Genius\\nAn AI-powered platform enabling interactive conversations with PDF\\ndocuments. Users can easily extract, query, and interact with content in their\\ndocuments.\\nExplore Docu Genius\\nUrdu Voice Chatbot\\nA dynamic chatbot designed to respond in Urdu, supporting both voice and\\ntext interactions. Perfect for engaging and intuitive communication in the\\nUrdu language.\\nTry Urdu Voice Chatbot\\nYT Converse\\nA platform that allows interactive chats with YouTube video content, offering\\ninsightful and engaging discussions based on the videoâ€™s information.\\nDiscover YT Converse\\nEXPERIENCE\\nCompleted Python programming assignments in basic\\nand advanced AI courses, enhancing coding and\\nproblem-solving skills. Gained hands-on experience in\\ndata analysis, visualization, and building interactive\\napps with Streamlit. These projects deepened my AI\\nknowledge and strengthened my problem-solving\\nabilities.\\nhttps://www.facebook.com/amminaiqball\\nAI , ML & Python\\nXSEVEN SOLUTION\\n2022 - 2024')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"data\\\\Blue Light Blue Color Blocks Flight Attendant CV.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "data = loader.load()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='AMINA IQBAL \n",
      "ML ENGINEER \n",
      "+923232983818 amminaiqball@gmail.com\n",
      "PORTFOLIO\n",
      "THANKS A LOT\n",
      "PROFILE' metadata={'source': 'Blue Light Blue Color Blocks Flight Attendant CV.pdf', 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.split_documents(data)\n",
    "# len(texts)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Replace with your actual OpenAI API key\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# embeddings = embed_model.embed_documents(str(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vector db search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Pinecone_class import pincoine_ob\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your index name cvdata-new Found'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pincoine_ob.check_index(\"cvdata-new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your index name cvdata-new Found'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(texts))\n",
    "# pdf_data = str(texts)\n",
    "# print(type(pdf_data))\n",
    "# print(pdf_data)\n",
    "\n",
    "pincoine_ob.check_index(index='cvdata-new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Data insert in cvdata-new successfully\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "res = pincoine_ob.insert_data_in_index(documents=texts,embeddings=embeddings,index_name='cvdata-new')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Name space data is Created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x23539dd33d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pincoine_ob.insert_data_in_namespace(documents= texts,embeddings = embeddings,index_name = \"cvdata-new\",name_space = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Data insert in cvdata-new successfully\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# pincoine_ob.insert_data_in_index(embeddings= embeddings,documents = texts,index_name = \"cvdata\")\n",
    "\n",
    "result = pincoine_ob.insert_data_in_index(documents=texts,embeddings=embeddings,index_name='cvdata-new')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = pincoine_ob.retrieve_from_namespace(index_name = \"cvdata\",embeddings = embeddings, name_space = \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIENCE\n",
      "Completed Python programming assignments in basic\n"
     ]
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
    ")\n",
    "docs = retriever.invoke(\"what student can do\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Student contact information is:\\nPhone number: +923232983818\\nEmail: amminaiqball@gmail.com\\nFacebook profile: https://www.facebook.com/amminaiqball'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain.invoke(\"what is student contact information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chain\n",
    "\n",
    "ðŸ‘‰ LLM ko direct prompt bhejne aur response lene ke liye\n",
    "\n",
    "âœ” Use case: Jab sirf LLM ka response chahiye without retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\"topic\": \"AI\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Chain (Simple RAG)\n",
    "\n",
    "ðŸ‘‰ LLM ko documents se relevant information retrieve kar ke response generate karwana\n",
    "\n",
    " Use case: Jab documents se relevant information extract karni ho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chain = retriever | llm\n",
    "print(chain.invoke(\"What is LangChain?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ 3. Stuff Retrieval Chain\n",
    "\n",
    "ðŸ‘‰ Multiple retrieved documents ko ek jagah combine kar ke use karna\n",
    "\n",
    "âœ” Use case: Jab multiple documents ko combine kar ke response generate karna ho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "combine_documents_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "chain = retriever | combine_documents_chain\n",
    "print(chain.invoke(\"What is LangChain?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ 4. Map-Reduce Retrieval Chain\n",
    "\n",
    "\n",
    "ðŸ‘‰ Agar documents bohot zyada hain to unko pehle summarize karna aur phir final answer dena\n",
    "\n",
    "âœ” Use case: Large document sets ke saath kaam karne ke liye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_map_reduce_documents_chain\n",
    "\n",
    "combine_documents_chain = create_map_reduce_documents_chain(llm)\n",
    "chain = retriever | combine_documents_chain\n",
    "print(chain.invoke(\"Explain deep learning?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ 5. Refine Retrieval Chain\n",
    "\n",
    "ðŸ‘‰ Pehle ek initial answer generate karna, phir usko refine karna\n",
    "\n",
    "âœ” Use case: Jab incrementally answer improve karna ho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_refine_documents_chain\n",
    "\n",
    "combine_documents_chain = create_refine_documents_chain(llm)\n",
    "chain = retriever | combine_documents_chain\n",
    "print(chain.invoke(\"Tell me about OpenAI?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ 6. Conversational Retrieval Chain (Memory-Based RAG)\n",
    "\n",
    "ðŸ‘‰ User ki pichli queries ka context yaad rakhna\n",
    "\n",
    "âœ” Use case: Jab chatbot ko user ka pichla conversation yaad rakhna ho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n",
    "print(chain.invoke({\"question\": \"Tell me about Tesla?\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ 7. Runnable RAG Chain (Pipelined Execution)\n",
    "\n",
    "ðŸ‘‰ LangChain ki new \"Runnable API\" ka use karna\n",
    "\n",
    "âœ” Use case: Jab custom modular pipeline chahiye jo scalable aur flexible ho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(chain.invoke(\"What is AI?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¹ 8. Multi-Retrieval Chain\n",
    "\n",
    "ðŸ‘‰ Agar multiple retrievers (like Pinecone + FAISS) use karne ho\n",
    "\n",
    "âœ” Use case: Jab multiple knowledge sources ko combine karna ho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MultiRetrievalQAChain\n",
    "\n",
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever2 = vectorstore2.as_retriever()\n",
    "\n",
    "chain = MultiRetrievalQAChain.from_retrievers(llm, retrievers=[retriever1, retriever2])\n",
    "print(chain.invoke({\"query\": \"Explain blockchain\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
